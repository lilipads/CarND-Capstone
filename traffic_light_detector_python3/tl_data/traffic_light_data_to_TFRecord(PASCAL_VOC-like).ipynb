{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Changing PASCAL VOC like dataset to TFRecord data.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import hashlib\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "from random import shuffle\n",
    "\n",
    "from lxml import etree\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "\n",
    "from object_detection.utils import dataset_util\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "SETS = ['training', 'validation']\n",
    "TYPES = ['sim', 'site']\n",
    "\n",
    "type_idx = 1\n",
    "data_type = TYPES[type_idx]\n",
    "root_dir = os.getcwd()\n",
    "data_dir = os.path.join(root_dir, data_type)\n",
    "relative_annotations_dir = '_data_labeled'\n",
    "output_path = root_dir + '/TFRecord'\n",
    "label_map_path = os.path.join(root_dir,'tl_label_map.pbtxt')  # Path to label map proto\n",
    "\n",
    "\n",
    "def dict_to_tf_example(data, dataset_directory, label_map_dict, set_name, ignore_difficult_instances=False):\n",
    "    \"\"\"Convert XML derived dict to tf.Example proto.\n",
    "\n",
    "    Notice that this function normalizes the bounding box coordinates provided\n",
    "    by the raw data.\n",
    "\n",
    "    Args:\n",
    "    data: dict holding PASCAL XML fields for a single image (obtained by\n",
    "      running dataset_util.recursive_parse_xml_to_dict)\n",
    "    dataset_directory: Path to root directory holding PASCAL dataset\n",
    "    label_map_dict: A map from string label names to integers ids.\n",
    "    ignore_difficult_instances: Whether to skip difficult instances in the\n",
    "      dataset  (default: False).\n",
    "    image_subdirectory: String specifying subdirectory within the\n",
    "      PASCAL dataset directory holding the actual image data.\n",
    "\n",
    "    Returns:\n",
    "    example: The converted tf.Example.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: if the image pointed to by data['filename'] is not a valid JPEG\n",
    "    \"\"\"\n",
    "    img_path = os.path.join(data_type + '_data', set_name, data['filename'])\n",
    "    full_path = os.path.join(dataset_directory, img_path)\n",
    "    with tf.gfile.GFile(full_path, 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = PIL.Image.open(encoded_jpg_io)\n",
    "    if image.format != 'JPEG':\n",
    "        raise ValueError('Image format not JPEG')\n",
    "    key = hashlib.sha256(encoded_jpg).hexdigest()\n",
    "\n",
    "    width = int(data['size']['width'])\n",
    "    height = int(data['size']['height'])\n",
    "\n",
    "    xmin = []\n",
    "    ymin = []\n",
    "    xmax = []\n",
    "    ymax = []\n",
    "    classes = []\n",
    "    classes_text = []\n",
    "    truncated = []\n",
    "    poses = []\n",
    "    difficult_obj = []\n",
    "    if 'object' in data:\n",
    "        for obj in data['object']:\n",
    "            difficult = bool(int(obj['difficult']))\n",
    "            if ignore_difficult_instances and difficult:\n",
    "                continue\n",
    "\n",
    "            difficult_obj.append(int(difficult))\n",
    "\n",
    "            xmin.append(float(obj['bndbox']['xmin']) / width)\n",
    "            ymin.append(float(obj['bndbox']['ymin']) / height)\n",
    "            xmax.append(float(obj['bndbox']['xmax']) / width)\n",
    "            ymax.append(float(obj['bndbox']['ymax']) / height)\n",
    "            classes_text.append(obj['name'].encode('utf8'))\n",
    "            classes.append(label_map_dict[obj['name']])\n",
    "            truncated.append(int(obj['truncated']))\n",
    "            poses.append(obj['pose'].encode('utf8'))\n",
    "\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(\n",
    "          data['filename'].encode('utf8')),\n",
    "        'image/source_id': dataset_util.bytes_feature(\n",
    "          data['filename'].encode('utf8')),\n",
    "        'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmin),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmax),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymin),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymax),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "        'image/object/difficult': dataset_util.int64_list_feature(difficult_obj),\n",
    "        'image/object/truncated': dataset_util.int64_list_feature(truncated),\n",
    "        'image/object/view': dataset_util.bytes_list_feature(poses),\n",
    "    }))\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    sets = ['training', 'validation']\n",
    "    \n",
    "    label_map_dict = label_map_util.get_label_map_dict(label_map_path)\n",
    "\n",
    "    for s in sets:\n",
    "        writer = tf.python_io.TFRecordWriter(os.path.join(output_path, s+'.record'))\n",
    "        print('Reading from traffic light {} {} dataset.'.format(data_type, s))\n",
    "        annotations_dir = os.path.join(data_dir, data_type + relative_annotations_dir, s)\n",
    "        annotations_path_list = glob.glob(annotations_dir + '/*.xml')\n",
    "        shuffle(annotations_path_list)\n",
    "        print('Number of dataset: {}'.format(len(annotations_path_list)))\n",
    "        for i, path in enumerate(annotations_path_list):\n",
    "            if i % 10 == 0:\n",
    "                print('On image {} of {}'.format(i, len(annotations_path_list)))\n",
    "            with tf.gfile.GFile(path, 'r') as fid:\n",
    "                xml_str = fid.read()\n",
    "            xml = etree.fromstring(xml_str)\n",
    "            data = dataset_util.recursive_parse_xml_to_dict(xml)['annotation']\n",
    "\n",
    "            tf_example = dict_to_tf_example(data, data_dir, label_map_dict, s)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "        writer.close()\n",
    "        print('{} {} dataset conversion done.'.format(data_type, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from traffic light site training dataset.\n",
      "Number of dataset: 384\n",
      "On image 0 of 384\n",
      "On image 10 of 384\n",
      "On image 20 of 384\n",
      "On image 30 of 384\n",
      "On image 40 of 384\n",
      "On image 50 of 384\n",
      "On image 60 of 384\n",
      "On image 70 of 384\n",
      "On image 80 of 384\n",
      "On image 90 of 384\n",
      "On image 100 of 384\n",
      "On image 110 of 384\n",
      "On image 120 of 384\n",
      "On image 130 of 384\n",
      "On image 140 of 384\n",
      "On image 150 of 384\n",
      "On image 160 of 384\n",
      "On image 170 of 384\n",
      "On image 180 of 384\n",
      "On image 190 of 384\n",
      "On image 200 of 384\n",
      "On image 210 of 384\n",
      "On image 220 of 384\n",
      "On image 230 of 384\n",
      "On image 240 of 384\n",
      "On image 250 of 384\n",
      "On image 260 of 384\n",
      "On image 270 of 384\n",
      "On image 280 of 384\n",
      "On image 290 of 384\n",
      "On image 300 of 384\n",
      "On image 310 of 384\n",
      "On image 320 of 384\n",
      "On image 330 of 384\n",
      "On image 340 of 384\n",
      "On image 350 of 384\n",
      "On image 360 of 384\n",
      "On image 370 of 384\n",
      "On image 380 of 384\n",
      "site training dataset conversion done.\n",
      "Reading from traffic light site validation dataset.\n",
      "Number of dataset: 45\n",
      "On image 0 of 45\n",
      "On image 10 of 45\n",
      "On image 20 of 45\n",
      "On image 30 of 45\n",
      "On image 40 of 45\n",
      "site validation dataset conversion done.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
